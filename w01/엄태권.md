# 01장. 신뢰할 수 있고 확장 가능하며 유지보수하기 쉬운 애플리케이션

오늘날 많은 애플리케이션은 `계산 중심` 과는 다르게 `데이터 중심` 적이다.

이러한 애플리케이션의 경우 보통 데이터의 양, 데이터의 복잡도, 데이터의 변화 속도가 큰 문제다.

일반적으로 데이터 중심 애플리케이션은 공통으로 필요하는 기능을 제공하는 표준 구성 요소로 만들며, 많은 애플리케이션은 다음을 필요로 한다.

```
- 나중에 다시 데이터를 찾을 수 있게 데이터를 저장(데이터베이스)
- 읽기 속도 향상을 위한 수행결과를 기억(캐시)
- 사용자가 키워드로 검색하거나, 다양한 방법으로 필터링 할 수 있도록 제공(검색 색인)
- 비동기 처리를 위해 다른 프로세스로 메시지 보내기(스트림 처리)
- 주기적으로 대량의 누적된 데이터를 분석(일괄 처리)
```

데이터 시스템이 성공적으로 추상화 되어있기 때문에 사실 위의 조건들이 뻔하게 들릴 수 있다.

그러나, 현실은 애플리케이션마다 요구사항이 다르기 때문에 어떤 도구와 어떤 접근 방식이 수행 중인 작업에 가장 적한한지 생각해야 하며, 단 하나의 도구만으로 할 수 없는 것을 해야 하는 경우 도구들을 결합하기 어려울 수도 있다.

### 데이터 시스템에 대한 생각

위에서 설명한 모든 것들을 왜 `데이터 시스템` 이라는 포괄적 용어로 묶어야 할까?

1. 데이터 저장과 처리를 위한 여러 새로운 도구는 최근에 만들어 졌으며, 이는 분류간 경계가 흐려져 더 이상 전통적인 분류에 딱 들어 맞지 않는다.(Ex. Redis, Kafka)
2. 점점 더 많은 애플리케이션이 단일 도구로는 더 잉상 데이터 처리와 저장 모두를 만족시킬 수 없는 과도하고 광범위한 요구사항을 갖고있다.

   ![image](https://github.com/akfls221/23-11-DesigningDataIntensiveApplications/assets/71249347/1e5cf3c2-4ebe-4f23-9d31-3c3860cf7705)

   작업은 효율적으로 수행할 수 있는 태스크로 나누고 다양한 도구들은 애플리케이션 코드를 이용해 서로 연결한다.


### 신뢰성

하드웨어나 소프트웨어 결함, 심지어 인적 오류 같은 역경에 직면하더라도 시스템은 지속적으로 올바르게 동작해야 한다. 즉, “무언가 잘못되더라도 지속적으로 올바르게 동작함”을 신뢰성의 의미로 이해할 수 있다.

잘못될 수 있는 일을 `결함(fault)` 라 부른다. 그리고 이러한 결함을 예측하고 대처할 수 있는 시스템을 `내 결함성(fault-tolerant)` 또는 `탄력성(resilient)` 를 지녔다고 말한다.

모든 종류의 걀함을 견딜 수 있는 시스템을 만들 수 있음을 시사하지만 실제로 실현 가능하진 않다.

`결함` 은 `장애(failoure)` 과 동일하지 않다. 장애는 사용자에게 필요한 서비스를 제공하지 못하고 시스템 전체가 멈춘 경우를 말한다. 결함을 0으로 줄이는 것은 사실상 불가능 하지만, 내결함성을 확보하도록 구조를 설계하는 것이 가장 좋다.

고의적으로 결함을 유도함으로써 내결함성 시스템을 지속적으로 훈련하고 테스트해서 결함이 자연적으로 발생했을 때 탄력적일 수 있다는 자신감을 높이는 것 또한 좋은 접근 방식이다.

### 하드웨어 결함

하드디스크의 편균 장애 시간은 약 10 ~ 50년으로 보고 됐다. 따라서 10,000개의 디스크로 구성된 저장 클러스터의 경우 평균적으로 하루에 한 개의 디스크가 죽는다고 예상해야 한다.

시스템 장애율을 줄이기 위한 첫 번째 대응으로 각 하드웨어 구성 요소에 중복을 추가하는 방법이 일반적이다. 디스크는 `RAID` 구성이 가능하고, `이중 전원 디바이스` 와 `핫 스왑` 가능한 CPU를 데이터 센터의 경우 건전지와 예비 전원용 디젤 발전기를 갖출 수 있다. 이런 접근 방식은 하드웨어 문제로 장애가 발생하는 것을 완전히 막을 수 는 없지만 보통 수년 간 장비가 중단되지 않고 계쏙 동작할 수 있게 한다.

하지만 데이터 양과 애플리케이션의 계산 요구가 늘어나면서 더 많은 수의 장비를 사용하게 됐고 이와 비례해 하드웨어 결함율도 증가했다. 또한 일부 클라우드 플랫폼은 가상 장비 인스턴스가 별도의 경고 없이 사용할 수 없게 되는 상황이 상당히 일반적이다.

따라서 소프트웨어 내결함성 기술을 사용하거나, 하드웨어 중복성을 추가해 전체 장비의 손실을 견딜 수 있는 시스템으로 점점 옮겨가고 있다.

### 소프트웨어 오류

위와 같은 하드웨어 결함과 또 다른 부류의 결함으로 `시스템 내 체계적 오류` 가 있다. 이 결함은 예상하기 더 어렵고 하드웨어 결함보다 오히려 시스템 오류를 더욱 많이 유발하는 경향이 있다.

```
- 잘못된 특정 입력이 있을 때 모든 애플리케이션 서버 인스턴스가 죽는 소프트웨어 버그
- 시스템의 속도가 느려져 반응이 없거나 잘못된 응답을 반환하는 서비스
- 한 구성 요소의 작은 결함이 다른 구성요소의 결함을 야기하는 연쇄 장애(cascading failure)
```

이 같은 소프트웨어 결함을 유발하는 버그는 특정 상황에 의해 발생하기 전까지 오랫동안 나타나지 않는다. 소프투웨어의 체계적 오류 문제는 신속한 해결책이 없으며, 시스템이 뭔가를 보장하길 기대한다면, 모니터링, 분석하기와 같은 여러 작은 일들을 수행중에 지속적으로 확인해 차이가 생기는 경우 경고를 발생시킬 수 있다.

### 인적 오류

사람이 미덥지 않음에도 시스템을 어떻게 신뢰성 있게 만들까? 최고의 시스템은 다양한 접근 방식을 결합한다.

```
- 오류의 가능성을 최소화하는 방향으로 시스템을 설계하라.
- 사람이 가장 많이 실수하는 장소(부분)에서 사람의 실수로 장애가 발생할 수 있는 부분을 분리하
  라 특히 실제 사용자에게는 영향이 없는 비프로덕션 샌드박스를 제공하라
- 단위 테스트부터 전체 시스템 통합 테스트와 수동 테스트까지 모든 수준에서 철저하게 테스트하라
- 장애 발생의 영향을 최소화하기 위해 인적 오류를 빠르고 쉽게 복구할 수 있게 하라.
- 성능 지표와 오류율 같은 상세하고 명확한 모니터링 대책을 마련하라.
- 조작 교육과 실습을 시행하라(onboarding)
```

### 확장성

확장성은 증가한 부하에 대처하는 시스템 능력을 설명하는 데 사용하는 용어지만 시스템에 부여하는 일차원적인 표식이 아님을 주의해야 한다. 확장성을 논한다는 것은 `시스템에 특정 방식으로 커지면 이에 대처하기 위한 선택은 무엇인가?`와 `추가 부하를 다루기 위해 계산 자원을 어떻게 투입할까?` 같은 질문을 고려한다는 의미다.

### 부하 기술하기

부하는 `부하 매개변수`라 부르는 몇 개의 숫자로 나타낼 수 있다. 가장 적합한 부하 매개변수 선택은 시스템 설계에 따라 달라진다. 부하 매개변수로 웹 서버의 초당 요청수, 데이터베이스의 읽기 대 쓰기 비율, 대화방의 동시 활성 사용자, 캐시 적중률 등이 될 수 있다.

트위터의 경우 확장성의 문제가 사실 쓰기부분(트윗 양)이 아닌, `팬 아웃(fan-out)` 때문이다.

트위터는 보통 사용자가 트윗을 작성하면, 해당 사용자를 팔로우 하고 있는 사람을 모두 찾고 각자의 홈 타임라인 캐시에 새로운 트윗을 삽입하여 읽기 요청에 대한 비용을 저렴하게 가져간다.

이 문제는 사실 많은 트윗을 보유하고 있는 사용자가 글을 썼을 경우 발생한다.(`유명인 문제`)

만일 일론 머스크가 글을 쓴다는 가정하에 일론 머스크의 단일 트윗이 홈 타임라인에 몇 천만 건 이상의 쓰기 요청이 될 지도 모른다는 의미다. 트위터의 경우 최종 전개는 2개의 접근 방식을 사용해 혼합형(hybrid)로 바꾸고 있다. 즉 대부분의 일반 사용자의 경우 fan-out 방식을 사요하지만 팔로워 수가 많은 소수 사용자(유명인)의 경우 fan-out 방식에서 제외한다. 즉, 팔로워가 많은 사용자의 경우 직접 DB에서 조회하여 fan-out으로 읽는 시점에 홈 타임라인에 합치는 작업을 한다. 이와 같은 혼합형 접근 방식은 좋은 성능으로 지속적인 전송이 가능하다.

### 성능 기술하기

일단 시스템 부하를 기술하면 부하가 증가할 때 어떤 일이 일어나는지 조사할 수 있다. 이러한 조사에는 성능 수치가 필요하다.

성능 수치에는 `처리량` 과 `응답 시간` 이 있다.

클라이언트가 몇번이고 반복해서 동일한 요청을 하더라도 매번 응답시간이 다르다. 그러므로 응답 시간은 숫자가 아닌 측정 가능한 값의 분포로 생각해야 한다.

보고된 서비스 평균 응답 시간을 살피는 일은 일반적이다. 하지만 전형적인 응답 시간을 알고 싶다면 평균은 그다지 좋은 지표가 아니다. 얼마나 많은 사용자가 실제로 지연을 경험했는지 알려주지 않기 때문이다.

일반적으로는 `백분위` 를 사용하는 편이 더 놓으며, 응답 시간 목록을 가지고 가장 빠른 시간부터 제일 느린 시간까지의 정렬에서 중간 지점을 `중앙값` 이라고 한다. 예를 들어 중간 응답 시간이 200밀리초면, 요청의 반은 그 미만, 나머지 반은 그보다 오래 걸린다는 뜻이다.

사용자가 보통 얼마나 오랫동안 기다려야 하는지 알고 싶다면 중앙값이 좋은 지표이며, 중앙 값은 `50분위` 로써 사용자가 여러 개의 요청을 보내면 최소한 하나의 요청이 중앙값보다 느릴 확률이 50%보다 훨씬 놓다.

만일 특이 값이 얼마나 좋지 않은지 알아보려면 상위 백분위를 살펴보는 방법도 좋다.(`p95, p99 p999` 예를 들어 95분위 응답 시간이 1.5초라면 100개의 요청중 95개는 1.5초 미만이고 100개의 요청중 5개는 1.5초보다 더 걸린다.

`꼬리 지연 시간`으로 알려진 상위 백분위 응답 시간은 서비스의 사용자 경험에 직접 영향을 주기 때문에 중요하다. 보통 응답 시간이 가장 느린 요청을 경험한 고객들은 많은 구매를 해서 고객 중에서 계정에 가장 많은 데이터를 갖고 있기 때문이다. 아마존의 경우 내부 서비스의 응답 시간 요구사항을 99.9분위로 기술하며, 99.99분위의 경우 최적화 작업에 비용이 너무 많이 들어 충분히 이익을 가져다주지 못한다고 여긴다.

![image](https://github.com/akfls221/23-11-DesigningDataIntensiveApplications/assets/71249347/f306cb44-d930-4699-84b4-6215a84eed93)

위와 같이 하나의 호출만으로도 전체 최종 사용자의 요청을 느리게 할 수 있다. 작은 비율의 백엔드 호출만 느려도 최종 사용자 요청이 여러 번 백엔드를 호출하면 느린 호출이 발생할 가능성이 증가한다. 그래서 최종 사용자 요청중 많은 비율의 응답 시간이 결국 느려진다.

예를 들어 백분위는 `서비스 수준 목표(SLO)` 와 `서비스 수준 협약서(SLA)` 에 자주 사용하고 기대 성능과 서비스 가용성을 정의하는 계약서에도 자주 등장한다.

`큐 대기 지연` 은 높은 백분위에서 응답 시간의 상당 부분을 차지한다. 서버는 병렬로 소수의 작업만 처리할 수 있기 때문에 소수의 느린 요청 처리만으로도 후속 요청 처리가 지체된다. 이 현상을 `선두 차단` 이라고 한다. 서버에서 후속 요청이 빠르게 처리되더라도 이전 요청이 완료되길 기다리는 시간 때문에 클라이언트는 전체적으로 응답 시간이 느리다고 생각할 것이다.

### 부하 대응 접근 방식

부하 매개변수가 어느정도 증가하더라도 좋은 성능을 유지하려면 어떻게 해야 할까

사람들은 확장성과 관련해 `용량 확장(scaling up)` 과 `규모 확장(scaling out)` 으로 구분해서 말하곤 한다. 이렇게 다수의 장비에 부하를 분산하는 아키텍처를 `비 공유` 아키텍처라고 부른다. 현실적으로 좋은 아키텍처는 실용적인 접근 방식의 조합이 필요하며, 예를 들어 적절한 사양의 장비 몇 대가 다량의 낮은 사양 가상 장비보다 훨씬 간단하고 저렴하다.

일부 시스템은 `탄력적`이어서 부하 증가를 감지하면 컴퓨팅 자원을 자동으로 추가할 수 있지만, 그렇지 않은 시스템은 수동으로 확장 해야 한다. 탄력적인 시스템은 부하를 예측할 수 없을 만큼 높은 경우 유용하지만 수동으로 확장한느 시스템이 더 간단하고 운영상 예상치 못한 일이 더 적다.

기존에는 이런 것들이 복잡하여 확장 비용이나 데이터베이스를 분산으로 만들어야 하는 고가용성 요구가 있을때까지 단일 노드에 데이터베이스를 유지하는 것이 통념이었으나, 일부 애플리케이션에서는 바뀌고 있으며, 이는 대용량 데이터와 트래픽을 다루지 않는 사용 사례에도 분산 데이터 시스템이 향후 기본 아키텍처로 자리 잡을 가능성이 있다.

특정 애플리케이션에 적합한 확장성을 갖춘 아키텍처는 주요 동작이 무엇이고 잘 하지 않는 동작이 무엇인지에 대한 가정을 바탕으로 구축한다.  이 가정은 곧 부하 매개변수가 되며, 이 가정이 잘못될 경우 최악의 경우 역효과를 낳는다. 즉 초기 단계나 검증되지 않은 제품의 경우 매를 가정한 부하에 대비해 확장하기 보단 빠르게 반복해 제품 기능을 개선하는 작업이 좀 더 중요하다.

### 유지보수성

소프트웨어 시스템상에서 일하는 많은 사람은 소위 `레거시` 시스템 유지보수 작업을 좋아하지 않는다.  모든 레거시 시스템은 각자 나름대로의 불편함이 있지만 희망적인 점은 유지보수 중 고통을 최소화 하고 레거시 소프트웨어를 직접 만들지 않게끔 소프트 웨어를 설계할 수 있다.

1. **운용성**

   운영팀이 시스템을 원활하게 운영할 수 있게 쉽게 만들어라
   운영 중 일부 측면은 자동화할 수 있고 또 자동화 해야한다. 다만 자동화를 처음 설정하고 제대로 동작하는지 확인하는 일은 여전히 사람의 몫이다. 좋은 운영성이란 동일하게 반복되는 태스크를 쉽게 수행 하게끔 만들어 운영팀이 고부가가치 활동에 노력을 집중한다는 의미다.


2. **단순성: 복잡도 관리**

    복잡도는 같은 시스템에서 작업해야 하는 모든 사람의 진행을 느리게 하고 나아가 유지보수 비용이 증가한다. 복잡도 수렁에 빠진 소프트웨어 프로젝트를 `커다란 진흙 덩어리` 로 묘사한다.

   복잡도는 다양한 증상으로 나타나며, 이러한 복잡도 때문에 시스템 유지보수가 어려울 때 예산과     일정이 초과되곤한다. 또한 복잡한 소프트웨어에서는 변경이 있을 때 버그가 생길 위험이 더 크다. 따라서 단순성이 구축하려는 시스템의 핵심 목표여야 한다.

   시스템을 단순하게 만드는 일이 반드시 기능을 줄인다는 의미는 아니며, `우발적 복잡도` 를 줄인다는 뜻에 가깝다. 우발적 복잡도 제거를 위한 최상의 도구는 `추상화` 이며, 이는 다른 다양한 애플리케이션에서도 재사용 할 수 있다. 이런 추상화는 큰 시스템의 일부를, 잘 정의되고 재사용 가능한 구성 요소로 추출할 수  있게 한다.


3. **발전성: 변화를 쉽게 만들기**

    시스템의 요구사항이 영원히 바뀌지 않을 가능성은 매우적다. 시스템의 요구사항이 끊임없이 변할 가능성이 훨씬 크다. 
    조직 프로세스 측면에서 `애자일` 작업 패턴은 변화에 적응하기 위한 프레임 워크를 제공한다. 또한 애자일 커뮤니티는 테스트 주도 개발 과 리팩토링 같이 자주 변화하는 환경에서 소프트웨어를 개발할 때 도움이 되는 기술 도구와 패턴을 개발하고 있다. 
    기본적으로 이런 애자일 기법에 대한 설명은 대부분 ㅈ매우 작고, 로컬 규모에 초점을 맞추고 있지만, 위에서 설명한 트위터의 아키텍처와 같은 대규모 데이터 세스템 수준에서의 민첩성(`발전성`)을 높이는 방법을 찾는 것이 중요하다.