# 08장. 분산 시스템의 골칫거리

## Overview

`어떤 것이든지 잘못될 가능성이 있다면 잘못된다.`

분산 시스템을 다루는 것은 한 컴퓨터에서 실행되는 소프트웨어를 작성하는 일과는 근본적으로 다르다. 그리고 핵심적인 차이는 뭔가 잘못될 수 있는 새롭고 흥미진진한 방법이 많다는 점이다.

엔지니어로서의 우리의 임무는 모든 게 잘못되더라도 제 역할을 해내는(사용자가 기대하는 보장을 만족시키는) 시스템을 구축하는 것이다.

## 결함과 부분 장애

단일 컴퓨터에서 실행되는 소프트웨어를 믿지 못할 근본적인 이유는 없다. 하드웨어가 올바르게 동작하면 같은 연산은 항상 같은 결과를 내며, 하드웨어 문제가 있으면 보통 시스템이 완전히 실패하는 결과를 낳는다.(보통 완전하게 동작 하거나 전체 장애가 발생하지 그 중간 상태가 되지는 않는다.)

반면 네트워크로 연결된 여러 컴퓨터에서 실행되는 소프트웨어를 작성할 때는 근본적으로 상황이 다르다. 분산 시스템에서는 시스템의 어떤 부분은 잘 동작하지만 다른 부분은 예측할 수 없는 방식으로 고장나는 것도 무리가 아니다.

이를 `부분장애` 라고 하며, 이는 `비결정적`이라서 어렵다. 어떨 때는 동작하지만 예측할 수 없는 방식으로 실패하기도 하고, 심지어 뭔가 성공했는지 아닌지 알지 못할 수 도 있다. 비결정성과 부분 장애 가능성이 분산 시스템을 다루기 어렵게 한다.

### 클라우드 컴퓨팅과 슈퍼컴퓨팅

대규모 컴퓨팅 시스템 구축 방법엔 보통 두 극단이 있다. `고성능 컴퓨팅` / `클라우드 컴퓨팅`

이런 철학에 따라 결함 처리 방법도 매우 다르다.

슈퍼컴퓨터의 경우 노드 하나에 장애가 발생했을 경우 전체 클러스터 작업부하를 중단하고 마지막 체크포인트 부터 계산을 재시작한다. `부분 장애를 전체로 확대`

우리가 다루는 인터넷 서비스를 구현하는 시스템은 이런 슈퍼컴퓨터와 매우 다르다.

- 애플리케이션은 언제라도 사용자에게 지연 시간이 낮은 서비스를 제공해야한다.(온라인)
- 상용 장비를 사용해 구축하며, 낮은 비용으로 동일한 성능을 제공하지만 실패율도 높다.
- 시스템이 커질수록 구성 요소 중 하나가 고장날 가능성이 높다. 오류 처리 전략에 그냥 포기하는 것을 포함한다면 대형 시스템은 결함으로부터 복구하는데 많은 시간을 쓰게 될 수 있다.
- 장애가 난 노드가 있어도 계속 동작할 수 있다면 이는 서비스를 계속 제공하면서 한 번에 노드 하나씩 재시작 하는 순회식 업그레이드가 가능하다.(성능이 낮은 장비를 죽이고 새 장비를 요청)
- 지리적으로 분산된 배포를 할때 통신은 로컬 네트워크에 비해 느리고 신뢰성도 떨어진다.

분산 시스템이 동작하게 만들려면 부분 장애 가능성을 받아들이고 소프트웨어에 내결함성 매커니즘을 넣어야 한다. 신뢰성 없는 구성 요소를 사용해 신뢰성 있는 시스템을 구축해야한다.

작은 시스템에서는 거의 항상 구성 요소 대부분이 올바르게 동작할 가능성이 높지만 조만간 시스템의 어떤 부분에 결함이 생길 것이고 소프트웨어는 어떤 식으로든 그 결함을 처리해야 한다.

결함이 드물 것이라 가정하고 최선의 상황을 바라는 것은 현명하지 못하며, 발생 가능성이 낮을지라도 생길 수 있는 결함을 광범위하게 고려해야 한다.(`테스트 환경에서의 인위적 상황까지 고려`)

## 신뢰성 없는 네트워크

인터넷과 데이터센터 내부 네트워크 대부분은 `비동기 패킷 네트워크` 이다. 이런 종류의 네트워크에서 노드는 다른 노드로 메시지(패킷)을 보낼 수 있지만 네트워크는 메시지가 언제 도착할지 혹은 메시지가 도착하기는 할 것인지 보장하지 않는다.

유일한 선택지는 수신 측에서 응답 메시지를 보내는 것이지만 응답 메시지도 손실되거나 지연될 수 있다. 유일한 정보는 응답을 아직 받지 못했다는 것이다. 다른 노드로 요청을 보내서 응답을 받지 못했다면 그 이유를 아는 것은 `불가능하다.`

이런 문제를 다루는 흔한 방법은 타임아웃이며, 얼마 간의 시간이 지나면 응답 대기를 멈추고 응답이 도착하지 않는다고 가정한다.

타임아웃이 발생했을 때 원격 노드가 응답을 받았는지는 알 수 없으며, 요청이 아직 어딘가의 큐에 들어있다면 전송 측은 요청을 포기했더라도 메시지가 수신 측에 도착할 수도 있다.

참고)

[메시지 전달 전략과 두 장군 문제(Message Delivery Semantics and Two Generals’ Problem)](https://monday9pm.com/메시지-전달-전략과-두-장군-문제-message-delivery-semantics-and-two-generals-problem-f8f1c7646c0b)

[Transactional Outbox Pattern 알아보기](https://velog.io/@eastperson/Transaction-Outbox-Pattern-알아보기)

### 현실의 네트워크 결함

누구도 네트워크 문제를 면할 수 는 없다. (`네트워크 결함` ) 드물더라도 `일어날 수 있다` 는 사실은 소프트웨어가 이를 처리할 수 있어야 한다는 뜻이다.

반드시 네트워크 결함을 견뎌내도록(`tolerating`) 처리할 필요는 없다. 평상시에는 상당히 믿을 만하다면 네트워크에 문제가 있을 때 사용자에게 오류를 보여주는 것도 타당한 방법이다.

그러나 소프트웨어가 네트워크 문제에 어떻게 반응하는지 알고 시스템이 그로부터 복구할 수 있도록 보장해야한다.(고의적인 네트워크 문제를 유발하고 테스트하는 것은 일리가 있다.)

### 결함 감지

많은 시스템은 결함 있는 노드를 자동으로 감지할 수 있어야 한다.

- 로드 밸런서는 죽은 노드로 요청을 그만 보내야 한다.(순번에서 빠진 것으로 간주해야 한다.)
- 단일 리더 복제를 사용하는 분산 데이터베이스에서 리더에 장애가 나면 팔로워 중 하나가 리더로 승격돼야 한다.

불행하게도 네트워크에 관한 불확실성 때문에 노드가 동작 중인지 아닌지 구별하기 어렵지만, 특정한 환경에서는 뭔가 동작하지 않는다고 명시적으로 알려주는 피드백을 받을 수 도 있다.

- 목적지 포트에서 수신 대기하는 프로세스가 없다면 `RST` 나 `FIN` 패킷으로 TCP연결을 닫거나 거부한다.
- 노드 프로세스가 죽었지만 노드의 운영체제는 실행 중이라면 스크립트로 프로세스가 죽었음을 알려 빠르게 역할을 넘겨받을 수 있게 한다.
- 데이터센터 내 네트워크 스위치의 관리 인터페이스에 접속이 가능하다면 질의를 보내 하드웨어 수준의 링크 장애를 감지할 수 있다.(`원격 장비의 전원이 내려갔는지`)
- 접속하려는 IP 주소에 도달할 수 없다고 라우터가 확신하면 ICMP 패킷으로 응답할 수 도 있다.

원격 노드가 다운되고 있다는 빠른 피드백은 유용하지만 의존할 수는 없다. 요청이 성공했음을 확신하고 싶다면 애플리케이션 자체로부터 긍정 응답을 받아야한다.

### 타임아웃과 기약 없는 지연

타임아웃이 길면 노드가 죽었다고 선언될 때까지 기다리는 시간이 길어진다. 짧아지면 결함을 빨리 발견하지만 일시적인 느림에도 죽었다고 잘못 선언할 위험이 높아진다.

성급하게 노드가 죽없다고 선언하면 문제가 된다. 노드가 실제로는 살아 있고 어떤 동작을 실행하는 중일 때 다른 노드가 역할을 넘겨 받으면 그 동작을 두 번 실행하게 될지도 모른다.

모든 패킷은 d 시간 내에 전송되거나 손실되지만 결코 d 보다 오랜 전송시간이 걸리지 않는다. 또 장애가 나지 않은 노드는 항상 요청을 r 시간내에 처리한다고 보장할 수 있다고 가정할 경우 성공한 요청은 모두 2d + r 시간 내에 응답을 받는다고 보장할 수 있다. 이게 사실이라면 2d + r을 타임아웃으로 사용하는게 합리적이다.

타임아웃이 낮으면 왕복 시간이 순간적으로 급증하기만 해도 시스템의 균형을 깨트린다.

### 네트워크 혼잡과 큐 대기

컴퓨터 네트워크에서 패킷 지연의 변동성은 큐 대기 때문인 경우가 많다.

- 네트워크 링크가 붐비면 패킷은 슬롯을 얻을 수 있을 때까지 잠시 기다려야 할 수 도 있다.
  네트워크는 잘 동작하고 있더라도 들어오는 데이터가 많아 큐를 꽉 채울 정도가 되면 패킷이 유실되어 재전송해야 한다.
- 장비의 부하에 따라 큐에서 대기하는 시간은 제가각일 ㅅ ㅜ있다.
- 가상 환경에서 실행되는 운영체제는 다른 가상 장비가 CPU 코어를 사용하는 동안 수십 밀리초 동안 멈출때가 흔하다.
- TCP는 `흐름제어` 를 수행한다. `혼잡 회피` 나 `배압` 이라고도 하는 흐름제어는 노드가 링크나 수신 노드에 과부하를 가하지 않도록 자신의 송신율을 제한하는 것이다.

공개 클라우드와 멀티 테넌트 데이터센터에서는 여러 소비자가 자원을 공유한다. 또한 일괄 처리 작업부하는 네트워크 링크를 포화시키기 쉽다. 이런 환경에서는 실험적으로 타임아웃을 선택하는 수밖에 없다.

지연의 변동성 측적을 위해 긴 기간에 여러 장비에 걸쳐서 네트워크 왕복 시간의 분포를 측정해야 한다. 그 후 장애 감지 지연과 너무 이른 타임아웃의 위험성 사이에서 적절한 트레이드오프를 결정할 수 있다. 더 좋은 방법은 고정된 타임아웃 대신 시스템이 지속적으로 응답 시간과 그들의 변동성을 측정하고 응답 시간 분포에 따라 동적으로 타임아웃을 조절하는 것이다.

### 동기 네트워크 대 비동기 네트워크

전화 네트워크는 극단적인 신뢰성을 지닌다. 이런 종류의 네트워크를 동기식이라고 하며, 이는 데이터가 여러 라우터를 거치더라도 큐 대기 문제를 겪지 않는다. 공간이 이미 할당됐기 때문이다.

그리고 큐 대기가 없으므로 네트워크 종단 지연 시간의 최대치가 고정돼 있다. 이를 `제한 있는 지연` 이라고 한다.

### 그냥 네트워크 지연을 예측 가능하게 만들 수는 없을까?

회선은 만들어져 있느 ㄴ동안 다른 누구도 사용할 수 없는 고정된 양의 예약된 대역폭이지만, TCP 연결의 패킷은 가용한 네트워크 대역폭을 기회주의적으로 사용한다.

그렇다면 왜 데이터센터 네트워크와 인터넷은 패킷 교환을 사용할까? 이들은 `순간적으로 몰리는 트래픽` 에 최적화됐기 때문이다. 웹 페이지 요청, 이메일 전송 등은 특별한 대역폭 요구사항이 없고 단지 가능하면 빨리 완료되기를 바랄 뿐이다.

순간적으로 몰리는 데이터 전송에 회선을 쓸 경우 네트워크 용량을 낭비하고 전송이 불필요하게 느려진다. 반대로 TCP는 가용한 네트워크 용량에 맞춰 데이터 전송률을 동적으로 조절한다.

서비스 품질과 진입 제어를 잘 쓰면 패킷 네트워크에서 회선 교환을 흉내 내거나 통계적으로 제한있는 지연을 제공하는 것도 가능하지만, 이런 서비스 품질은 인터넷 통신 등에서 사용할 수 없다.

현재 배포된 기술로는 네트워크의 지연과 신뢰성에 대해 어떤 보장도 할 수 없으며, 기약없는 지연이 발생할 것이라고 가정해야 한다. 결과적으로 타임아웃에 `올바른 값` 은 없으며 실험을 통해 결정해야 한다.

## 신뢰성 없는 시계

분산 시스템에서는 통신이 즉각적이지 않으므로 시간은 다루기 까다롭다. 메시지가 네트워크를 거쳐서 한 장비에서 다른 장비로 전달되는 데 시간이 걸린다. 메시지를 받은 시간은 항상 보낸 시간보다 나중이지만, 얼마나 나중일지는 알 수 없다.

게다가 네트워크에 있는 개별 장비는 자신의 시계를 갖고 있다. 시간을 어느 정도 동기화할 수 있지만(NTP) 각 장비는 자신만의 시간 개념이 있으며 이는 다른 장비보다 약간 빠를 수도 느릴 수도 있다.

### 단조 시계 대 일 기준 시계

현대 컴퓨터는 최소 두 가지 종류의 시계를 갖고 있다. 일 기준시계와 단조 시계다.

**[일 기준 시계]**

직관적으로 시계에 기대하는 일을 한다. 어떤 달력에 따라 현재 날짜와 시간을 반환한다. 에포크는 그레고리력에 따르면 UTC 1970년 1월 1일 자정을 가리킨다. 보통 NTP로 동기화 한다.

일 기준 시간은 로컬 시계가 NTP서버보다 너무 앞서면 강제로 리셋되어 과거 시점으로 돌아가는 것처럼 보이는 이상한 점이 있다.

**[단조 시계]**

타임아웃이나 서비스 응답 시간 같은 지속 시간(시간 구간)을 재는 데 적합하다. 예를 들면 리눅스의 clock_gettime과 자바의 System.nanoTime()이 있다.

두 대의 다른 컴퓨터에서 나온 단조 시계 값을 비교하는 것은 의미가 없으며 이들은 동일한 것을 의미하지 않기 때문이다.

분산 시스템에서 경과 시간을 재는 데 단조 시계를 쓰는 것은 일반적으로 괜찮다. 다른 노드의 시계 사이에 동기화가 돼야 한다는 가정이 없고 측정이 약간 부정확해도 민감하지 않기 때문이다.

### 시계 동기화 정확도

단조 시계는 동기화가 필요 없지만 일 기준 시계는 ntp 서버나 다른 외부 시간 출처에 맞춰 설정돼야 유용하다.

유감스럽게도 시계가 정확한 시간을 알려주게 하는 방법은 기대만 큼 신뢰성이 잇거나 정확하지 않다. NTP또한 변덕이 생길 수 있다.

NTP 데몬 설정이 잘못되거나 방화벽이 NTP 트래픽을 차단하면 드리프트에 따른 시계 오류는 빠르게 커질 수 있다.

### 동기화된 시계에 의존하기

앞선 설명과 마찬가지로 시계 또한 결함이 발생할 수 있으며 이런 결함을 처리해야한다. 대부분의 시간에 아주 잘 동작하지만 견고한 소프트웨어는 잘못된 시계에 대비할 필요가 있다.

한 가지 문제는 시계가 잘못된다는 것을 눈치채지 못하기 쉽다는 것이다. CPU나 네트워크가 잘못 설정되면 전혀 동작하지 않을 가능성이 높아서 빨리 발견되고 수리될 것이다.

반면 NTP 클라이언트가 잘못 설정됐다면 시계는 드리프트가 생겨서 점점 실제 시간으로부터 멀어져 가지만 대부분이 잘 동작하는 것처럼 보인다.

따라서 동기화된 시계가 필요한 소프트웨어를 사용한다면 필수적으로 모든 장비 사이의 시계차이를 모니터링 해야한다. 다른 노드우ㅘ 시계가 너무 차이나는 노드는 죽은 것으로 선언되고 클러스터에서 제거 돼야 한다.

### 이벤트 슨서화용 타임스탬프

클라이언트 A가 노드 1에 X = 1을 쓴다. 그  쓰기는 노드 3으로 복제되고, 클라이언트 B가 노드 3에 있는 X를 1증가 시킨다.(X=2) 그리고 마지막으로 두 쓰기는 노드 2로 복제된다.

![image](https://github.com/Learning-Is-Vital-In-Development/23-11-DesigningDataIntensiveApplications/assets/71249347/519037fe-26d6-4fc6-9ec8-aaae3f411bb7)

X = 2가 분명 나중에 쓰여졌지만, X = 1을 쓰는 타임스탬프는 42.004초이고 X = 2를 쓰는 타임스탬프는 42.003초다. 노드 2가 두 이벤트를 받을 때 X = 1이 더 최근 값인 것으로 잘못 결정해서 X = 2로 쓴 값을 버리게 된다. 이 충돌 해소 전략은 최종 쓰기 승리(LWW) 라고 불린다.

타임스탬프를 서버가 아닌 클라이언트에서 생성하는 구현도 있지만 LWW에 존재하는 근본적인 문제를 바꾸지는 못한다.

따라서 `최근` 의 정의는 로컬 일 기준 시계에 의존하며 그 시계는 틀릴 수도 있다는 것을 아는 게 중요하다. 엄격하게 동기화된 시계를 쓰더라도 타임스탬프 100밀리초에 패킷을 보내고 수신측의 시계에 따라 타임스탬프 99초에 패킷을 받을 수도 있다.

올바른 순서화를 위해서는 시계 출처가 측정하려고 하는 대상(네트워크 지연) 보다 훨씬 더 정확해야 한다. 이른바 논리적 시계는 진동하는 수정 대신 증가하는 카운터를 기반으로 하며 이벤트 순서화의 안전한 대안이다.

`논리적 시계`는 일 기준 시간이나 경과한 초 수를 측정하지 않고 이벤트의 상대적인 순서, 한이 벤트가 다른 이벤트의 앞이나 뒤에 일어났는가만 측정한다. 반대로 일 기준 시계와 단조 시계는 실제 경과 시간을 측정하며 `물리적 시계`라고도 한다.

### 시계 읽기는 신뢰 구간이 있다

시계 읽기를 어떤 시점으로 생각하는 것은 타당하지 않다. 어던 신뢰 구간에 속하는 시간의 범위로 읽는게 나을 것이다. 예를 들어 어떤 시스템은 현재 시간이 해당 분의 10.3초와 10.5초 사이에 있다고 95% 확신할 수 있으나 그보다 더 정확히 알지 못할 것이다.

대부분의 시스템은 이런 불확실성을 노출하지 않는다. 예를 들어 clock_gettime() 호출 시 해당 반환값은 타임스탬프의 예상 오차를 말해주지 않으므로 신뢰 구간이 얼마인지 모른다.

예외적으로 구글 트루타임 API가있으며, 이 API는 로컬 시계의 신뢰 구간을 명시적으로 보고한다.

현재 시간을 요청하면 가능한 타임스탬프 범위 중 `가장 이른 것` 과 `가장 늦은 것`을 가리키는 두 개의 값을 받는다.

시계는 불확실 성 계산을 기반으로 실제 현재 시간이 그 구간 안의 어딘가에 있다는 것을 알며, 그 구간의 너비는 무엇보다도 로컬 수정 시계가 더 정확한 시계 출처와 마지막으로 동기화된 이후로 얼마나 지났는지에 의존한다.

### 전역 스냅숏용 동기화된 시계

스냅숏 결니는 작고 빠른 읽기 쓰기 트랜잭션과 오래 실행되는 읽기 전용 트랜잭션 모두를 지원해야하는 유용한 기능이다.

이것은 잠금을 쓰지 않고 읽기 쓰기 트랜잭션을 방해하지 않으며서 읽기 전용 트랜잭션이 특정 시점의 일관적인 상태에 있는 데이터베이스를 볼 수 있게 한다.

단일 노드 데이터베이스에서는 단순한 카운터가 트랜잭션 ID를 생성하는데 충분하지만 여러 데이터 센터에 있는 여러 장비에 분산돼 있을 때는 코디네이션이 필요하다. 작고 빠른 트랜잭션이 많으면 분산 시스템에서 트랜잭션 ID 생성은 방어할 수 없는 병목이 된다.

스패너의 스냅숏 구현은 트루타입 PAI가 보고한 시계 신뢰 구간을 사용하며 다음과 같은 관찰을 기반으로 한다.  각각 가장 이른 타임스탬프와 가장 늦은 타임스탬프를 포함하는 두 개의 신뢰 구간이 있고(A = [가장 이른 A, 가장 늦은 A]) 와 B[가장이른 B, 가장 늦은 B]) 라는 두 구간이 겹치지 않는다면

(가장이른A < 가장늦은 A < 가장이른 B < 가장늦은B) B는 분명히 A 보다 나중에 실행됐다고 본다.

트랜잭션 타임스탬프가 인과성을 반영하는 것을 보장하기 위해 스패너는 읽기 쓰기 트랜잭션을 커밋하기 전에 의도적으로 신뢰 구간의 길이만큼 기다린다. 그렇게 하면 그 데이터를 읽을지도 모르는 트랝개션은 충분히 나중에 실행되는 게 보장되므로 신뢰 구간이 겹치지 않는다.