# 들어가기에 앞서

- 현업에서 일어나는 문제점들을 맛보고 우리가 기댈 수 있는 것과 그렇지 않은 것을 살펴본다.
- 엔지니어의 임무는 모든게 잘못되더라도 제역햘을 해내는 (사용자가 기대하는) 시스템을 구축하는 것.→ 9장에서는 분산시스템에서 이런보장을 제공하는 알고리즘의 몇가지 예를 살펴본다.
- 분산 시스템에서 잘못될지도 모르는 것에 관한 네트워크 관련 문제, 시계및 타이밍 문제를 조사하고 이것들을 어느정도로 회피 할 수 있는지 설명한다.
- 분산 시스템의 상태에 대해 생각하는 방법과 무슨 일이 일어났는지 추론하는 방법을 알아본다.

# 결함 부분과 장애

단일 컴퓨터에서 실행되는 소프트웨어를 믿지 못할 근본적인 이유는 없다. 

하드웨어가 올바르게 동작하면 같은 연산은 항상 같은 결과를 낸다. 좋은 소프트웨어가 설치된 컴퓨터는 보통 완전하게 동작하거나 전체 장애가 발생하지 그 중간 상태가 되지는 않는다. 
→ 예) 하드웨어문제(메모리 오염/헐거운 커넥터) 가 있으면 보통 시스템이 완전히 실해하는 결과를 낳는다. 

네트워크로 연결된 여러 컴퓨터에서 실행되는 소프트웨어를 작성 할 때는 근본적으로 상황이 다르다. 

`분산시스템`에서는 더 이상 이상화 된 시스템 모델에서 동작하지 않는다. 물리적 세계의 잘못일 뿐! → 아래 일화로 설명

→ 예) 저혈당증을 앓는 운전자가 자신의 포드 픽업 트럭을 DC의 HVAC(난방,통풍,공조) 시스템에 처박은 적이 있어 전체 DC전원 장애 발생

> 비결정성과 부분 장애 가능성이 분산시스템을 어렵게 만드는 요인
> 

이처럼 분산 시스템에서는 시스템의 어떤 부분은 잘 동작하지만 다른 부분은 예측할 수 없는 방식으로 고장난다. 

이를 `**부분 장애**(partial failure)`이라고 한다. 부분 장애는 비결정적이라서 어렵다.

여러 노드와 네트워크와 관련된 뭔가를 시도하면 어떨 때는 동작하지만 어떨 때는 예측 할 수 없는 방식으로 실패한다. 

심지어 뭔가 성공했는지 실해했는지 알지 못할 수도 있다. **→ 메시지가 네트워크를 거쳐 전송되는 시간도 비결정적이기 때문에!**

## 클라우드 컴퓨팅과 슈퍼 컴퓨팅

> 대규모 컴퓨팅 시스템 구축 방법 (2)
→ 전통적 데이터센터는 두 극단의 중간 지점에 있음
> 
- `**고성능 컴퓨팅`** ( high-performance computing HPC)분야
    - 수천개의 CPU를 가진 슈퍼 컴퓨터는 보통 일기예보 같은 계산 비용이 매우 높은 과학 계산 작업에 쓰인다.
    - 실행되는 작업은 보통 계산 상태를 지속성 있는 저장소에 체크포인트로 저장한다.
        - 노드 하나에 장애가 발생했을 때 흔한 해결책은 그냥 전체 클러스터 작업부하를 중단하는 것 (부분 장애를 전체 장애로 확대하는 방법으로 문제 해결)
        - 따라서 분산시스탬보다는 단일 노드 컴퓨터에 가깝다.
- `**클라우드 컴퓨팅**`
    - 멀티 테넌트 데이터센터 IP, 네트워크(이더넷 ethernet)로 연결된 상용(comodity) 컴퓨터, 신축적(elastic)/주문식(on-demand) 자원할당, 계량 결제(metered billing)등과 관련이 있다.
    - 분산 시스템이 동작하게 만들려면 부분 장애 가능성을 받아들이고 소프트웨어에 내결함성 메커니즘을 넣어야한다. 바꿔 말하면 신뢰성 없는 구성요소를 사용해 신뢰성 있는 시스템을 구축해야된다.
        - 6장 신뢰성에서 설명했듯이 완벽한 신뢰성이란 없으므로 현실적으로 보장할 수 있는 제약을 이해해야한다.
    

단지 몇개의 노드만으로 구성된 작은 시스템이라도 부분장애를 고려하는 것은 매우 중요하다.

- 작은 시스템은 거의 항상 올바르게 동작할테지만 어떤 부분에 결함이 생기게 되면 
소프트웨어는 어떤 식으로 결함을 처리해야되기 때문에 결함이 발생할 것을 대비해 소프트웨어가 어떻게 동작할것인지 알아야한다.
- 분산시스템에서 비관주의, 의심, 편집증은 그 값어치를 한다!

> **신뢰성 없는 구성요소를 사용해 신뢰성 있는 시스템 구축하기 (277pg)**
잘 이해가 안돼에에에
> 
> - 518pg 종단간 논증에 대해서 더 알아봄

# 신뢰성 없는 네트워크

분산시스템은 `비공유 시스템`, 즉 네트워크로 연결된 다수의 장비를 듯한다. 

- 네트워크는 장비들이 통신하는 유일한 수단!
- 각 장비는 자신만의 메모리와 디스크를 가지고 있다.
- 다른 장비의 메모리나 디스크에 접근할 수 없다고 가정한다.

<aside>
💡 `**비공유**`가 **`시스템을 구축하는 유일한 방법**은` 아닌데 왜 주된 방법이 되었나? 
- 특별한 하드웨어가 필요하지 않아 상대적으로 저렴한 **클라우드 서비스 이용** 가능!
- 지리적으로 분산된 여러 데이터센터에 중복 배치함으로써 **높은 신뢰성 확보** !

</aside>

`비동기 패킷 네트워크(asynchronous packet network)` 

인터넷과 데이터 센터 내부 네트워크 대부분(흔히 이더넷)은 비동기 패킷 네트워크이다.

- 이런 류의 네트워크에서 노드는 다른 노드로 메시지(패킷)을 보낼 수 있지만
- 네트워크는 메시지가 언제 도착할지 혹은 메시지가 도착하기는 할것인지 보장하지 않는다. 요청을 보내고 응답을 기다릴때 여러 문제가 발생할 수 있음
    
    ![1000048834.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/fe5d7f0e-3b33-4513-9b73-373a7fef9157/9486e4e0-c428-41da-8bac-ec58e818042b/1000048834.jpg)
    
- 비동기 패킷 네트워크에서 전송측은 패킷이 전송됐는지 아닌지 조차 구별할 수 없다. 
수신 측에서 응답 메시지를 보낼 수 있지만 응답 메시지도 손실되거나 지연될 수 있다.

→ 해결방법 : `타임아웃`

- 얼마간의 시간이 지나면 응답 대기를 멈추고 응답이 도착하지 않았다고 가정한다.
- 그러나 타임아웃 발생시 원격 노드가 응답을 받았는지 아닌지도 여전히 알 수 없다.
    - 요청이 아직 어딘가의 큐에 들어가있다면 전송 측은 요청을 포기했더라도 메시지가 수신 측에 도착할 수도 있다.

## 현실의 네트워크 결함

수십년동안 컴퓨터 네트워클르 구축해왔지만 신뢰성 있는 네트워크를 만드는데 아직 성공하지 못했다.

데이터센터처럼 제어된 환경에서도 네트워크 문제가 발생한다.

- 단일 장비의 연결이 끊어지는 경우
- 전체 랙의 연결이 끊어지는 경우
- 랙 상위 스위치 (top of rack switch)
- 집계 스위치 (aggregation switch)
- 로드밸런서 구성요소 장애

→ 네트워크 중단 주요 원인이 **인적오류**(예:스위치 설정 오류) 이기 때문에 네트워크 장비를 중복 추가해도 결함이 줄어들지는 않음 

EC2같은 클라우드 서비스는 일시적인 네트워크 결함이 자주 발생하는 것으로 악명이 높기 때문에 잘 관리된 비공개 데이터 센터가 더 안정적인 환경이 될 수 있다.

하지만 어떤 환경에서도 네트워크 문제를 면할 수 없다. 

- 스위치의 소프트웨어 업그레이드 중 생기는 문제는 네트워크 토폴로지 재구성을 유발할 수 있곡, 그동안 네트워크 패킷이 1분이상 지연될 수 있다.
- 상어가 해저 케이블 물어뜯기
- 수신 패킷은 모두 누락하지만 송신 패킷은 잘보내는 네트워크 인터페이스가 있다.
    - 네트워크 링크가 한방향으로 동작한다고 해서 반대방향도 동작하리라고 보장되는 것은 아니다.

> `네트워크 분단`
네트워크 결함 때문에 네트워크 일부가 다른 쪽과 차단되는 것을 `네트워크 분단(network partition)`이나 `네트워크 분리(netsplit)`이라고 한다. 이책에서는 6장에서 살펴본 저장 시스템의 파티션(샤드)과 혼동하지 않도록 더 일반적인 용어 `**네트워크 결함(network fault)**`를 사용
> 

네트워크 결함의 오류 처리가 저으이되고 테스트 되지 않는다면 나쁜일이 제멋대로 생길 수 있다. 

- 예 :  클러스터가 교착 상태에 빠져 네트워크가 복구되더라도 영구적으로 요청을 처리할 수 업게 될 수 있으며, 심지어 모든 데이터를 지워버릴 수 있다.
- 소프트웨어는 예상하지 못한 상황에 빠지면 제멋대로 예측못하는 일을 할지도 모른다.

해결 방법 

- 반드시 네트워크 결함을 견뎌내도록 (tolerating) 처리 할 필요는 없다.
- 네트워크가 평상시에 상당히 견고하다면 네트워크 문제 있을 때 그냥 사용자에게 오류 메세지 보내주는 것도 타당한 방법
- 고의로 네트워크 문제를 유발하고 시스템 반응 테스트도 일리가 있다! (`**카오스몽키** (chaos monkey)`)

## 결함 감지

많은 시스템은 결함 있는 노드를 자동으로 감지할 수 있어야한다. 

- 로드밸런서는 죽은 노드로 요청을 그만보내야한다. (죽은 노드는 순번에서 빠진것으로 간주!)
- 단일 리더 복제를 사용하는 분산 데이터 베이스에서 리더에 장애가 나면 팔로워 중 하나가 리더로 승격돼야 한다. (158pg 노드 중단 처리)

하지만! 네트워크에 대한 불확실성 때문에 이 노드가 동작 중인지 죽었는지 구별이 힘들다. 
따라서 어떤 특정 환경에서는 뭔거 동작하지 않는다고 명시적으로 알려주는 피드백을 받을 수 있다.

요청이 성공 했음을 확신하고 싶다면 애플리케이션 자체로부터 긍정응답을 받아야한다.

- 원격 노드가 다운되고 있다는 빠른 피드백은 유용하지만 여기에 의존할 수 없다.
- TCP가 패킷이 전달됐다는 확인 응답(ask)을 했더라도 애플리케이션이 그것을 처리하기 전에 죽을 수도 있다.

역으로 뭔가 잘못되면 스택의 어떤 수준에서 오류응답을 받을지도 모르지만 일반적으로 아무 응답을 받지 못항거라고 가정해야된다. 

- 몇번 재시도 해보고 (TCP는 사용자 모르게 재시도를 하지만 애플리케이션 수준에서 재시도 가능) 타임 아웃 만료 되기를 기다림
- 타임아웃 내에 응답을 ㅂ다지 못하면 마침내 노드가 죽었다고 선언한다.

## 타임아웃과 기약없는 지연

<aside>
💡 그렇다면 위에서 지속적으로 말했다싶이 `타임아웃`이 만료되기를 기다리는 것이 
결함을 감지하는 확실한 수단이라면 `타임아웃은 얼만큼 길어야하는 것일까`?
- **타임 아웃이 길때**  : 노드가 죽었다고 선언될때까지 기다리는 시간이 길어지고 사용자들은 오류메시지를 계속 봐야함
- **타임 아웃이 짧을 때**  : 결함은 빨리 발견하지만 노드가 일시적으로 느려진건데(네트워크 부하 급증) 죽었다고 잘못선언할 확률 증가

</aside>

노드가 죽었다고 선언되면 그 노드의 책무는 다른 노드로 전달되어야 해서 다른 노드와 네트워크에 추가적인 부하를 준다.
시스템이 이미 높은 부하에 허덕이는 중이라면 성급하게 노드가 죽었다고 선언하는 것은 문제를 악화시킨다.

그 노드의 부하를 다른 노드로 전달하면 연쇄 장애를 유발할 수 있기 때문에다. 극단적인 경우 모든 노드들이 서로를 죽었다고 선언해서 모든 것이 중단될 수 있다. 

> **`타임아웃 :** 2d+r (합리적)`
패킷의 최대 지연시간이 보장된 네트워크를 사용하는 가상의 시스템을 상상해보자.
> 
> - 모든 패킷은 어떤 시간 d 내에 전송되거나 손실되지만 결코d보다 전송시간이 오래걸리지는 않는다.
> - 장애가 나지 않은 노드는 항상 요청을 r 시간 내에 처리한다고 보장함을 가정한다.

하지만 우리가 사용하는 시스템은 대부분 이중 어떤것도 보장하지 않는다. 

`비동기 네트워크`는 `**기약없는 지연(unbounded delay)**`가 있다.

- 패킷을 가능한 빨리 보내려고하지만 패킷이 도착하는데 걸리는 시간에 상한치는 없다.

서버 구현은 대부분 어떤 최대 시간 내에 요청을 처리한다고 보장할 수 없다. 

타임아웃이 낮으면 **왕복 시간(round trip time)**이 순간적으로 급증하기만 해도 시스템의 균형을 깨뜨린다.

## 네트워크 혼잡과 큐 대기

자동차 운전 시 도로 네트워크에서 이동하는 시간은 대부분 교통체증에 따라 달라지는 경우가 많은 것 처럼

컴퓨터 네트워크에서 패킷 지연의 변동성은 큐 대기 때문인 경우가 많다.

![1000048835.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/fe5d7f0e-3b33-4513-9b73-373a7fef9157/7c01a67d-3537-4d5d-b0e9-a013bd0a7d3d/1000048835.jpg)

게다가 `**TCP**`는 어떤 타임아웃(왕복 시간을 관찰해서 계산한다.) 안에 확인 응답을 받지 않으면  패킷이 손실됐다고 간주하고 손실된 패킷은 자동으로 재전송한다. 

패킷 손실이나 재전송이 보이지 않지만 그 결과로 생기는 지연은 보인다. 

- 지연 예시 : 타임아웃이 만료되기를 대기하고 재전송된 패킷이 확인 응답을 받기를 대기하는 것으로 보인다.

네트워크 지연의 변동성에 영향을 주는 다양한 요인

- 큐 대기 지연은 시스템이 최대 용량에 가까울 때 특히 광범위하게 일어난다.
- 예비 용량이 풍부한 시스템은 쉽게 큐를 비울 수 있지만 사용률이 높은 시스템은 긴 큐가 매우 빨리 만들어진다.

공개 클라우드와 멀티 테넌트 데이터 센터는 여러 소비자가 자원을 공유한다. 

- 네트워크 링크와 스위치 그리고 각 장비의 네트워크 인터페이스와 CPU도 공유하게 된다.
- 맵리듀스 같은 일괄처리 작업부하는 네트워크 링크를 포화시키기 쉽다.
- 공유된 자원을 다른 사용자가 사용하는 것을 제어하거나 간파할 수 없으므로 자원을 많이 사용하는 누군가가(`시끄러운 이웃`)가 가까이 있다면 네트워크 지연변동이 클 수 있다.
    - 이런 환경에서 실험적으로 타임아웃을 선택해야된다.
    - 지연의 변동성이 얼마나 되는지 알려면 ,  긴 기간 내에 여러 장비에 걸쳐서 네트워크 왕복 시간의 분포를 측정해야된다.
    - 그후 애플리케이션의 특성을 고려해서 **장애 감지 지연**과 **너무 이른 타임아웃의 위험성 사이**에서 적절한 **트레이드 오프**를 결정할 수 있다.
    - `더좋은 방법!`
        - 고정된 타임아웃을 설정하는 대신 시스템이 지속적으로 응답 시간과 그들의 변동성(지터 jitter)을 측정하고 관찰된 응답 시간 분포에 따라 타임아웃을 자동으로 조절하게 하는 것이다.
        - 파이 증가 장애 감지기(phi accrual failure detector)를 쓰면 된다. (예: `아카`akka, `카산드라`, TCP 재전송 타임아웃도 유사)

## 동기 네트워크 대 비동기 네트워크

패킷 전송 지연 시간의 최대치가 고정되어 있고 패킷을 유실하지 않는 네트워크를 사용할 수 있다면 분산 시스템은 훨씬 단순해진다!

<aside>
💡 왜 하드웨어 수준에서 이 문제를 해결하고 네트워크를 신뢰성 있게 만들어서 소프트웨어 단에서 걱정할 수 없게 만들수 없을까?

답: 데이터 센터 네트워크를 전통적인 고정 회선전화 네트워크가 비교해보면 된다!
전화 네트워크는 극단적인 신뢰성을 지닌다. 음성 프레임이 지연되거나 통화가 유실되는 일은 매우 드물다. 
전화 통화는 꾸준히 종단(end to end)지연 시간이 낮아야하며 목소리 음성 샘플을 전송할 대역폭이 충분해야된다. 

이처럼 컴퓨터 네트워크에서도 비슷한 신뢰성과 예측 가능성이 있다면 좋지 않을까?
전화 네트워크에서 통화를 할 때는 회선(circuit)이 만들어진다. 통활ㄹ 하는 두명 사이에 있는 전체 경로를 따라서 그 통화에 대해 고정되고 보장된 양의 대역폭 ~~~ 
이런 종류의 네트워크는 `동기식`이다. 데이터가 여러 라우터를 거치더라도 큐 대기 문제를 겪지 않는다. 
네트워크의 다음 홉(hop)에 통화당 16비트의 공간이 이미 할당됐기 때문이다. 그리고 큐대기가 없으므로 네트워크 종단 지연 시간의 최대치가 고정되어있다. 이를 제한 있는 지연(bounded delay)라고 한다.

</aside>

## 그냥 네트워크 지연을 예측 가능하게 만들수는 없을까?

전화 네트워크의 회선은 TCP 연결과 매우 다르다는 점을 주목해야한다.

`회선` 

- 만들어져있는 동안 다른 누구도 사용할 수 없는 고정된 양의 예약된 대역폭

`TCP 연결 패킷`

- 가용한 네트워크 대역폭을 기회주의적으로 사용한다.
- TCP에 가변 크기의 데이터 블록(이메일, 웹페이지)를 보면 가능하면 짧은 시간 안에 전송하려고 한다.
- TCP 연결이 유휴 상태에 있는 동안은 어떤 대역폭도 사용하지 않는다.

데이터 센터 네트워크와 인터넷이 `회선교환(circuit -switch)`네트워크라면 회선이 구성됐을 때 왕복 시간의 최대치를 보장할 수 있다.

하지만 이더넷과 IP는 큐 대기의 영향을 받는 `패킷 교환(packet-switch)`프로토콜이고 따라서 네트워크에 기약없는 지연이 있다. 

→ 이들 프로토콜에는 회선의 개념이 없다.

> 왜 데이터 센터 네트워크와 인터넷은 패킷 교환을 사용할까?
> 
> - 순간적으로 몰리는 트래픽(bursty traffic)에 최적화 됐기 때문에.
>     - `회선`
>         - 통화를 하는 동안 보내는 초당 비트 개수가 상당히 고정돼있는 음성/영상통화에 적합
>         - 회선을 통해 파일 전송하고 싶으면 대역폭 할당을 추정해야한다. 
>         추정치가 너무 낮으면 네트워크 용량을 쓰지 않고 남겨 둔 상태로 전송이 불필요하게 느려진다.
>         추정치가 너무 높으면 회선은 구성될 수 없다.(회선교환 네트워크는 대역폭 할당을 보장할 수 없다면 회선 생성을 허용하지 않기 때문)
>         
>         → 따라서 **순간적으로 몰리는 데이터 전송**에 회선을 쓰면 네트워크 용량을 낭비하고 전송이 불필요하게 느려진다.
>         
>     - `패킷`
>         - 웹페이지 요청, 이메일 전송, 파일 전송은 특별한 대역폭 요구사항이 없다. 단지 빨리 완료되는 것이 중요
>         - TCP는 가용한 네트워크 용량에 맞춰 데이터 전송률을 동적으로 조절한다.
>     - ATM처럼 회선 교환과 패킷 교환을 모두 지원하는 `하이브리드 네트워크`
>         - 인피니밴드(infiniBand) 유사
>         - 링크 계층에서 종단 흐름제어를 구현해서 네트워크에서 큐대기를 할 필요성을 줄이지만 링크 혼잡 때문에 여전히 지연의 영향을 받을 수 있다.
>         - `서비스 품질(quality of service,QoS)`과 `진입제어(admission control)`를 잘 쓰면 패킷 네트워크에서 회선 교환을 흉내내거나 통계적으로 제한있는 지연을 제공하는 것도 가능하다.

그러나 이런 서비스 품질은 현재 멀티 테넌트 데이터센터와 공개 클라우드에서 사용할 수 없고 인터넷 통해 통신할 때도 사용할 수 없다.

현재 배포된 기술로는 네트워크의 지연과 신뢰성에 대해 어떤 보장도 할 수 없다. 

### 결론!

**네트워크 혼잡, 큐 대기, 기약 없는 지연이 발생할 것이라고 가정해야된다. 결과적으로 타임아웃에 `‘올바른’ 값은 없으며 실험을 통해 결정`해야한다.!!!**

# 신뢰성 없는 시계

시계와 시간은 중요하다. 애플리케이션은 다음과 같은 질문에 대답하기 위해 다양한 방식으로 시계에 의존한다. 

- 타임아웃됐나? (`지속시간` : `요청을 보낸 시점과 응답을 받는 시점 사이의 시간 구간`)
- 99분위 응답시간은 어떻게 되나? (`지속시간`)
- 이 캐시 항목은 언제 만료되나? (`시점` : `특정 날짜의 특정 시간에 발생한 이벤트`)
- 며칠 몇시에 미리 알림 이메일을 보내야하나? (`시점`)

분산 시스템에서는 통신이 즉각적이지 않으므로 시간은 다루기 까다롭다. 

- 메시지가 네트워크를 거쳐서 한 장비에서 다른 장비로 전달되는데 시간이 걸린다.
- 메시지를 받은 시간은 항상 보낸 시간보다 나중이지만 네트워크의 지연의 변동성 때문에 얼마나 나중일지는 알 수 없다.

네트워크에 있는 개별 장비는 자신의 시계를 갖고 있다.

- 실제 하드웨어 장치로 보통 `수정 발진기 (quartz crystal oscillator)` 다.
- 이 장치는 시간이 정확하지 않아 자신만의 시간 개념이 있으며 이는 다른 장비보다 약간 빠를 수도 느릴 수도 있다. 시간을 어느 정도 동기화 활 수 있다. 가장 널리 쓰이는 메커니즘은 네트워크 시간 프로토콜(network time protocol NTP) 서버 그룹에서 보고한 시간에 따라 컴퓨터 시간 조정이 가능하다.

## 단조 시계 대 일 기준 시계

현대 컴퓨터는 최소 두가지 종류의 시계를 갖고 있다. 

일 시준 시계(time-of-day-clock)

- 직관적으로 시계에 기대하는 일을 한다. 어떤 달력에 따라 현재 날짜와 시간을 반환한다.
- 에포크(epoch)이래로 흐른 초 수를 반환
    - 리눅스 gettime(clock_realtime)
    자바 System.currentTimeMillis()
- 역사적으로 매우 거친(coarse-grained)`해상도`를 가진다. 
*`해상도`? 해상도는 얼마나 촘촘하게 데이터를 수집하는지를 의미한다.
    - 오래된 윈도우 시스템에서는 10밀리초 단위로 흐른다. 최근 시스템에서는 이게 별 문제가 되지 않는다.

단조 시계 (monotonic clock)

- 타임아웃이나 서비스 응답 시간 같은 지속 시간(시간 구간)을 재는데 적합하다. → 항상 앞으로 흐른다는 사실에서 단조시계라는 명칭이 나옴 (일시계는 시간이 거꾸로 뛸 수 있다.
    - 리눅스 clock_gettime(CLOCK_MONOTONIC)
    자바 System.nanoTime()
- 한 시점에서 단조 시계의 값을 확인하고 어떤 일을 한 후 나중에 다시 시계를 확인할 수 있다. 
두값 사이의 차이로 두번의 확인 사이에 시간이 얼마나 흘렀느지 확인 가능하다.
- 그러나 시계의 절대적인 값은 의미가 없다.
    - 컴퓨터가 시작한 이래 흐른 나노초 일수도 있고 비슷한 어떤것일수도 있음
    - 특히 두대의 다른 컴퓨터에서 나온 단조시계 값을 비교하는 것은 의미가 없다. 이들을 동일하지 않기 때문

여러개의 `CPU소켓`이 있는 서버는 CPU마다 독립된 타이머가 있을 수 있다. (타이머는 반드시 다른 CPU와 동기화 되는것은 아님)

운영체제는 이 차이를 보정해서 애플리케이션 스레드가 여러 CPU에 걸쳐 스케쥴링 되더라도 시계가 단조적으로 보이게 하려고 한다. 

- 하지만! 단조성 보장은 곧이 곧대로 받아들이지 않는게 현명하다.

`NTP`는 컴퓨터 로컬 시계가 NTP 서버보다 빠르거나 느리다는 것을 발견하면 단조 시계가 진행하는 진도수를 조정할 수 있다. (시계를 돌린다 `slewing`)라고 한다.) 

분산 시스템에서 경과 시간을 재는 데 단조시계를 쓰는것은 일반적으로 괜찮다.

 다른 노드의 시계 사이에 동기화가 돼야 한다는 가정이 없고 측정이 약간 부정확해도 민감하지 않기 때문이다.

## 시계 동기화와 정확도

`단조시계` : 동기화 필요 x

`일 기준 시계` : NTP서버다 다른 외부 시간 출처에 맞게 설정되어야 유용함.

시계 정확도가 중요해서 상당한 자원을 투입할 생각이 있다면 시계 정확도를 매우 높이는 것도 가능하다.

- 유럽 금융 기고나 규제 초안은 고빈도 트레이딩 펀드는 모두 그들의 시계를 UTC와 100마이크로초 이내로 동기화 하기를 요구하는데, 이는 플래시 크래시(flash crashes) 같은 시장 이상 현상을 디버깅하고 시장 조작을 감지하는데 도움되게 만들기 위해서다.

`GPS 수신기`, `정밀 시간 프로토콜 (Precision Time Protocol ,PTP)` 과 세심한 배포 및 모니터링을 사용해서 달성할 수 있다.

## 동기화된 시계에 의존하기

시계 동기화가 잘못될 수 있는 수많은 방법이 있다.

- 하루는 정확히 86,400초가 아닐 수 있다.
- 일 기준 시계가 거꾸로 갈수 있다.
- 노드의 시간이 다른 노드의 시간과 차이가 많이 날 수 있다.

이번 장 초반부 패킷을 유실하고 임의로 지연시키는 네트워크에 대해 얘기했다. 네트워크가 대부분의 시간에 잘동작하더라도 소프트웨어는 네트워크에 결함이 생길 수 있다는 가정하게 설계돼어야 하며 소프트 웨어는 이런 결함을 우아하게 처리해야한다. 

시계도 마찬가지이다. 대부분의 시간에 아주 잘 동작하지만 견고한 소프트웨어는 잘못된 시계에 대비할 필요가 있다. 

한가지 문제는 장비의 CPU결함 혹은 네트워크의 문제는 빨리 발견되고 수리되지만 시계는 아니라는 것이다. 

장비의 수정 시계에 결함이 있거나 NTP 클라이언트가 잘못 설정 됐다면 시계는 드리프트가 생겨서 점점 실제 시간으로부터 멀어져 가지만 대부분이 잘 동작하는 것 처럼 보인다. 

→ 극적인 고장보다는 미묘한 데이터 손실 발생

## 이벤트 순서화용 타임스탬프

예를 들어 두 클라이언트가 분산 데이터베이스에 쓰면 누가 먼저 쓰게 될까? 누가 쓴게 더 최근이 될까?

![IMG_4876A407B29F-1.jpeg](https://prod-files-secure.s3.us-west-2.amazonaws.com/fe5d7f0e-3b33-4513-9b73-373a7fef9157/d9c70106-1cef-4842-8f01-26c2fc00f56a/IMG_4876A407B29F-1.jpeg)

이 충돌 해소 전략은 `**최종 쓰기 승리(last write wins, LWW)`** 

- 다중 리더 복제와 카산드라와 리악 같은 리더 없는 데이터 베이스에서 널리 사용된다. (187pg 최종 쓰기 승리(동시 쓰기 버리기) 참고)

타임스탬프를 서버가 아니라 클라이언트에서 생성하는 구현도 있지만 LWW에 존재하는 근본적인 문제를 바꾸지는 못한다. 

- 데이터베이스 쓰기가 불가사의하게 사라질 수 있다. 시계가 뒤처지는 노드는 시계가 빠른 노드가 먼저 쓴 내용을 그들 사이에 차이나는 시간이 흐를 때까지 덮어쓸 수 없다. 이 시나리오 애플리케이션에는 어떤 오류도 보고 되지 않지만 임의의 양의 데이터가 조용히 유실되는 문제를 유발할 수 있다.

가장 “최근” 값을 유지하고 다른 것들을 버림으로써 충돌을 해소하고 싶은 유혹이 들더라도 

“최근”의 정의는 `로컬 일 기준 시계`에 의존하며 그 시계는 틀릴 수도 있다는 것을 아는게 중요하다. 

<aside>
💡 잘못된 순서화가 발생하지 않도록 NTP동기화를 정확하게 할 수 있을까? 
아마도 불가능할 것이다. NTP정확도 자체가 시계 드리프트 같은 다른 오류 요인 외에도 네트워크 왕복 시간에 따라 제한되기 때문이다.
올바른 순서화를 위해서는 시계 출처가 측정하려고 하는 대상(즉 네트워크 지연)보다 훨씬 더 정확해야한다. 
→ `논리적 시계(logical clock)`는 진동하는 수정(quartz crystal) 대신 증가하는 카운터를 기반으로 하며 **이벤트 순서화의 안전한 대안**이다. 
(186 동시쓰기 참고)

`논리적 시계`는 일 기준 시간이나 경과한 초 수를 측정하지 않고 `이벤트의 상대적인 순서`(이벤트가 다른 이벤트의 앞이나 뒤에 일어났는가)만 측정한다.
반대로 **일기준시계**와 **단조시계**는 실제 경과 시간을 측정하며 `물리적 시계(physical clock)`라고 한다.

</aside>

## 시계 읽기는 신뢰 구간이 있다.

불확실성 경계는 시간 출처를 기반으로 계산할 수 있다.

GPS 수신기나 컴퓨터에 직접 부착된 원자 시계가 있으면 제조사에서 제공하는 예상 오류 범위가 있다. 

시간을 서버로 부터 얻는다면 불확실성은 서버와 마지막으로 동기화한 시간 이후로 예상되는 시계 드리프트에 NTP 서버의 불확실성을 더하고 그 서버와 통신할 때 걸리는 네트워크 왕복 시간을 더한 값을 기반으로 한다. (첫번째 근사치로 서버를 믿을 수 있다고 가정)

불행하게도 대부분의 시스템은 이불확실성을 노출하지 않지만 예외가 있다.

스패너(spanner)에 있는 `구글 트루타임(truetime) API`다.

- 로컬 시계의 신뢰구간을 명시적으로 보고 한다.
- 현재 시간 요청 시 가능한 타임스탬프 범위 중 가장 이른것, 가장 늦은 것을 가리키는 [earliest, latest] 받는다.
- 그 구간의 너비는 로컬 수정 시계가 더 정확한 시계 출처와 마지막으로 동기화 된 이후로 얼마나 지났는지에 의존한다.

## **전역 스냅숏용 동기화된 시계**

스냅숏 격리에 대해서 236pg ‘스냅숏 격리와 반복 읽기’에서 설명했다. 
스냅숏 격리는 **작고 빠른 읽기 쓰기 트랜잭션 & 크고 오래 실행되는 읽기 전용 트랜잭션(백업/분석용)** 모두를 지원해야 하는 **데이터 베이스**에서 유용하다.

- 잠금을 쓰지 않고 읽기 쓰기 트랜잭션을 방해하지 않으면서
- 읽기 전용 트랜잭션이 특정 시점의 일관적인 상태에있는 데이터베이스를 볼 수 있게한다.

가장 흔한 스냅숏 격리 구현은 단조 증가하는 트랜잭션 ID가 필요하다. 

그러나 여러 DC에 있는 여러 장비에 분산되어 있는 DB라면 전역 단조 증가 트랜잭션 ID를 생성하기 어렵다.

트랜잭션 타임스탬프가 인과성을 반영하는 것을 보장하기 위해 

- 스패너는 시계 신뢰 구간을 이용하여 타임스탬프를 트랜잭션 ID로 쓴다. →  구간이 겹치지 않으면 인과성을 판단할 수 있다.
- 겹치는 경우에만 실행 순서를 확신할 수 없는데, 이를 위해 읽기 쓰기 트랜잭션을 커밋하기 전에 의도적으로 신뢰 구간의 길이만큼 기다린다.
- 그렇게 하면 그 데이터를 읽을지도 모르는 트랜잭션은 충분히 나중에 실행되는 게 보장되므로 신뢰 구간이 겹치지 않는다.
- 대기 시간을 가능하면 짧게 유지하기 위해 스패너는 시계 불확실성을 가능하면 작게 유지해야한다.

→ 이런 목적으로 구글은 각 데이터센터에 GPS수신기나 원자시계를 배치해서 시계가 약 7밀리초 이내로 동기화 되게함

## **프로세스 중단**

프로그램 실행 중 예상치 못한 중단이 있었다면, 중단 이후로 권한이 없지만 프로세스가 뭔가 안전하지 않은 일을 한 상태일지 모른다.(lease 예시)

아주 오랫동안 멈추는 경우는 다양하다.

- stop-the-world
- 가상 장비 suspend & resume
- 노트북 덮개 닫기
- context switching (steal time)
- I/O 연산
- Swap to disk (thrashing)
- SIGSTOP

단일 장비에서는 뮤텍스, 세마포어 등 다양한 도구로 thread-safe를 구현하지만, 분산 시스템에서는 바로 변형해서 사용할 수 없다.

분산 시스템의 노드는 어느 시점에 실행이 상당한 시간 동안 멈출 수 있다고 가정해야 한다.

## **응답 시간 보장**

스레드와 프로세스는 기약없는 딜레이 unbounded delay가 일어날 수 있지만 노력하면 중단의 원인을 제거할 수 있다.

심각한 손상을 유발할 수 있는(mission-critical한) **소프트웨어는 응답해야 하는 deadline**이 명시되고, 이를 만족하지 못하면 장애가 발생될 수 있다.

이런 시스템을 `**엄격한 실시간 시스템 hard real-time system**`이라고 한다.

<aside>
💡 실시간은 정말 실시간인가? 각각의 실시간의 정의
**임베디드 시스템**
- 시스템이 명시된 타이밍 보장을 모든 상황에서 만족하도록 신중하게 설계되고 테스트 됐다.
**웹**
- 임베디드보다 모호하게 사용함 , 서버가 클라이언트에게 데이터를 푸시하고 엄격한 응답 시간 제약없이 스트림 처리하는 것

</aside>

예) 자동차의 차내 센서가 지금 충돌이 날 것을 감지했다면 에어백 방출 시스템에서 좋지 않은 시점에 발생한 GC중단 때문에 에어백이 늦게 방출되기를 바라지 않을것이다.

- 실시간 보장을 제공하려면, 프로세스가 명시된 간격의 CPU 시간을 할당받을 수 있게 보장되도록 스케줄링해 주는 RTOS(real time operating system)가 필요하다.
- 라이브러리 함수는 최악의 실행 시간을 문서화해야한다.
- 동적 메모리 할당은 제한되거나 완전히 금지될 수 있다.(실시간 가비지 컬렉터도 있지만 애플리케이션은 여전히 GC가 많은일을 하지 않도록 보장해야함.)
- 실시간 시스템은 매우 많은 비용이 들고, 고성능이 아닐 수도 있으며, 언어 및 도구가 제한된다.
- 대부분 서버 시스템에게 실시간 보장은 전혀 경제적이지도, 적절하지도 않기 때문에 중단과 시계 불안정으로부터 고통받을 수 밖에 없다.

## **가비지 컬렉션의 영향을 제한하기**

GC 중단을 노드가 잠시동안 계획적으로 중단되는 것으로 간주하고 노드가 GC 하는 동안 클라이언트로부터 요청을 다른 노드들이 처리하게 하는 것이다.

이런 트릭은 GC 중단을 클라이언트로부터 감추고 응답 시간의 상위 백분위를 줄여준다.

이 아이디어의 변종은 컬렉션을 빨리 할 수 있는 수명이 짧은 객체만 가비지 컬렉터를 사용하고 수명이 긴 객체의 전체 GC가 필요할 만큼 객체가 쌓이기 전에 주기적으로 프로세스를 재시작하는 방법이다.
